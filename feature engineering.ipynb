{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It’s also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value.\n",
    "For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. One way to deal with this is to transform the output target in order to tame the\n",
    "magnitude of the growth. **Log transformation** is a good way to handle this.\n",
    "\n",
    "it is usually necessary to prune the input features using automatic **feature selection**. Feature selection is the process of reducing the number of input variables when developing a predictive model.\n",
    "\n",
    "* Scalar = A single numeric feature\n",
    "* Vector = A list of numeric features\n",
    "* Space = Vectors sit in a space\n",
    "\n",
    "\n",
    "**Robustness** means that the method works under a wide variety of conditions.\n",
    "\n",
    "To **quantize** data means to convert it from a continuous to a discrete form. This is often done to reduce the complexity of a model.\n",
    "For example, [1, 1221, 1281729127] can be quantized to [0, 1, 2]. To avoid losing other important information because od the large difference between the values of that specific feature.\n",
    "\n",
    "* Fixed-width binning: Split the range of the data into N fixed-width bins\n",
    "* Quantile binning: adaptively position the bins based on the distribution of the data\n",
    "\n",
    "\n",
    "**Log function** is used for transformation frequently, it maps:\n",
    "* (0,1) to (-inf,0)\n",
    "* (1, 10) to (0,1)\n",
    "* (10, inf) to (1, inf)\n",
    "\n",
    "_While log transformations, do not forget to +1 if 0 is one of the possible values for the field, as it explodes to the infinity_\n",
    "\n",
    "i.e. it compresses the range of large values and expands the range of small values."
   ],
   "id": "36a2ddb05c514158"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Log transformation** is a member of **variance-stabilizing transformations.** For example, suppose a random variable X has the Poisson\n",
    "distribution. If we transform X by taking its square root, the variance of X˜ = X is roughly constant, instead of being equal to the mean.\n",
    "\n",
    "\n",
    "The combination of log and root transformation is defined as the **Box-Cox transformation**. It is defined as:\n",
    "\n",
    "\\begin{cases}\n",
    "\\frac{x^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0, \\\\\n",
    "\\ln(x), & \\text{if } \\lambda = 0.\n",
    "\\end{cases}\n",
    "\n",
    "Only positive values are allowed. The parameter λ is chosen to maximize the normality of the resulting distribution. This is done by applying the Kolmogorov-Smirnov test for normality to the transformed data.\n",
    "\n",
    "---\n"
   ],
   "id": "75f9ea4086b8bab6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scaling\n",
    "\n",
    "**Min-Max scaling** is the simplest scaling method. It scales the data to a fixed range [0, 1]. The formula is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Standardization** scales the data to have a mean of 0 and a variance of 1. It assumes that the data is normally distributed. The formula is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "X_{std} = \\frac{X - \\mu}{\\sigma}\n",
    "\\end{equation}\n",
    "\n",
    "where μ is the mean of the feature values and σ is the standard deviation.\n",
    "\n",
    "**L2** normalization is a vector norm that is often used in machine learning. It is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "X_{norm} = \\frac{X}{||X||_2}\n",
    "\\end{equation}\n",
    "\n",
    "where ||X||2 is the L2 norm of the feature vector X. The L2 norm is calculated as the square root of the sum of the squared vector values.\n",
    "\n",
    "\\begin{equation}\n",
    "||X||_2 = \\sqrt{\\sum_{i=1}^{n} X_i^2}\n",
    "\\end{equation}\n",
    "\n",
    "Basically, after L2 normalization, the feature column has a unit norm (the sum of the squares of the elements is 1). -> The values are placed on the ***unit circle***.\n",
    "\n",
    "Feature scaling is useful in situations where a set of input features differs wildly in scale. As, it does not affect distribution of the data, it is a **preprocessing** step.\n",
    "\n",
    "---"
   ],
   "id": "e31d5a51b1f831ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Interaction Features\n",
    "\n",
    "Interaction features are new features that are obtained by taking the product or some other interaction between two or more variables. They are used in linear models to capture the effect of two or more variables acting together.\n",
    "\n",
    "For example, if the output of the linear regression is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "y = w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "\\end{equation}\n",
    "\n",
    "and the interaction feature that allows to capture the effect of two variables acting together is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "y = w_1x_1 + w_2x_2 + ... + w_nx_n + w_{1,1}x_1x_1 + w_{1,2}x_1x_2 + ...\n",
    "\\end{equation}\n",
    "\n",
    "---"
   ],
   "id": "f58a5ae997c3742b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bag of Words\n",
    "\n",
    "Bag of Words is a method to represent text data when modeling text with machine learning algorithms. It involves two things:\n",
    "1. A vocabulary of known words.\n",
    "2. A measure of the presence of known words.\n",
    "\n",
    "In a bag of words, each word becomes a dimension of the vector. Though, it is a simple and effective way to represent text data, it has the main disadvantage of losing the semantic meaning of the text.\n",
    "For example, “toy dog” and “dog toy” could be very different things."
   ],
   "id": "b1bf9f45f1cc273c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bag-of-n-Grams\n",
    "Is a natural extension of *Bag of Words*. Instead of just counting individual words, it counts sequences of words. The word sequences are called n-grams. For example, in the sentence “The cat in the hat”, the 2-grams are “The cat”, “cat in”, “in the”, and “the hat”.\n",
    "\n",
    "- Captures the context of the words\n",
    "- Expensive in terms of memory and computation\n",
    "\n",
    "***Stemming*** is the process of reducing a word to its base or root form. For example, “running” is the root form of “runs”, “ran”, and “running”. It is used to reduce the number of unique words in the vocabulary."
   ],
   "id": "3b262acdb849538f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TF-IDF\n",
    "Is a measure of how important a word is in a document. It is calculated as the product of two terms: the term frequency and the inverse document frequency.\n",
    "\n",
    "The term frequency is the frequency of a word in a document. It is calculated as:\n",
    "\n",
    "$$\n",
    "TF = \\frac{\\text{Number of times the word appears in the document}}{\\text{Total number of words in the document}}\n",
    "$$\n",
    "\n",
    "The inverse document frequency is the frequency of the word in the entire corpus. It is calculated as:\n",
    "\n",
    "$$\n",
    "IDF = \\log{\\frac{\\text{Total number of documents}}{\\text{Number of documents with the word in it}}}\n",
    "$$\n",
    "\n",
    "Finally, the TF-IDF score is calculated as:\n",
    "\n",
    "$$\n",
    "TF-IDF = TF \\times IDF\n",
    "$$"
   ],
   "id": "6dc2216570a04201"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is a technique used to prevent overfitting by artificially penalizing model complexity.\n",
    "The regularization parameters and hyperparameters are not learned automatically. They are set manually before the learning process begins.\n",
    "\n",
    "***Grid Search*** is a technique used to find the optimal hyperparameters for a model. It works by defining a grid over the hyperparameters and searching for the best combination of hyperparameters in the grid.\n",
    "\n",
    "---"
   ],
   "id": "f1a6863e197b3bda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Categorical Variables\n",
    "\n",
    "Categorical variables are variables that take on a limited, and usually fixed, number of possible values. But they are ***non-ordinal***, meaning that there is no inherent order to the categories.\n",
    "\n",
    "## One-Hot Encoding\n",
    "Categories are converted to binary vectors. Each category is mapped to a binary vector where all elements are zero except for the element corresponding to the categories, which is one.\n",
    "\n",
    "$$\n",
    "e_1 + e_2 + ... + e_n = 1\n",
    "$$\n",
    "\n",
    "That is, the value represents one of the categories. That would be the problem because of the linear dependency between the columns. The trained models will not be unique.\n",
    "\n",
    "## Dummy Encoding\n",
    "To remove extra degree, one category is mapped to the vector of 0's. That is known as ***reference category***.\n",
    "\n",
    "## Effect Encoding\n",
    "Similar to dummy encoding, but the reference category is encoded as -1 instead of 0. This is useful when the reference category is expected to have a negative effect on the output. It is also known as **Deviation Encoding**.\n",
    "\n",
    "---\n"
   ],
   "id": "7ab13d5e7955e355"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dealing with Large Categorical Variables\n",
    "\n",
    "Since real world data often results in high cardinality categorical variables, it is important to know how to deal with them.\n",
    "\n",
    "1. Do nothing special, and feed linear models with one-hot encoded features.\n",
    "2. Compress the features\n",
    "    - **Feature hashing**: Hash the categories into a fixed number of buckets. This is a lossy process, as multiple categories can be mapped to the same bucket.\n",
    "    - **Bin counting**: Used for high cardinality categorical variables. It counts the number of times each category appears in the dataset.\n",
    "\n",
    "## Feature Hashing\n",
    "A hash function is a deterministic function that maps a potentially unbounded integer to a finite integer range [1, m]. Since the input domain is potentially larger than the output range, multiple numbers may get mapped to the same output. This is called a ***collision***. A ***uniform hash function*** ensures that roughly the same number of numbers are mapped into each of the m bins.\n",
    "\n",
    "## Bin Counting\n",
    "The idea of bin counting is deviously simple: rather than using the value of the categorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical\n",
    "value, we compute the association statistics between that value and the target that we wish to predict.\n",
    "\n",
    "In short, bin counting converts categorical data of the variable to the statistics about the value.\n",
    "\n",
    "---"
   ],
   "id": "5b056fd72ccb7de4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dimensionality Reduction - PCA\n",
    "\n",
    "***Principal Component Analysis (PCA)*** is a technique used to reduce the dimensionality of the data. It works by finding the directions of maximum variance in the data and projecting the data onto a smaller subspace of that variance.\n",
    "\n",
    "PCA focuses on the notion of linear dependency. We describe the column space of the data matrix as the span of all feature vectors. if the column space is small compared to the amount of features, then most of the features are linearly dependent and need to be removed.\n",
    "\n",
    "The key idea here is to replace redundant features with a few new features that adequately summarize information contained in the original feature space.\n",
    "\n",
    "Statistical quantities, like variance and expectation are defined in terms of the data distribution. In real life, we do not have a true distribution, but only observed points $z_1, z_2, ...$. That is called ***empirical distribution***.\n",
    "\n",
    "$$\n",
    "Var_{emp}(Z) = \\frac{1}{n-1} \\sum_{i=1}^{n} z_i^2\n",
    "$$\n",
    "\n",
    "We want to maximize the variance of the projected data. The variance of the projected data is given by:\n",
    "\n",
    "$$\n",
    "Var_{emp}(Zw) = \\frac{1}{n-1} \\sum_{i=1}^{n} (z_i^Tw)^2\n",
    "$$\n",
    "\n",
    "This formulation of PCA presents the target more clearly: we look for an input direction that maximizes the norm of the output.\n",
    "\n",
    "$$\n",
    "w_{PCA} = argmax_{||w||=1} Var_{emp}(Zw)\n",
    "$$\n",
    "\n",
    "The answer lies in the ***Single Value Decomposition (SVD)*** of the data matrix. SVD is a matrix factorization technique that factors a matrix into three matrices. The SVD of a matrix X is given by:\n",
    "\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where U and V are orthogonal matrices and Σ is a diagonal matrix of singular values. The columns of U are the principal components of X. The first principal component is the column of U corresponding to the largest singular value. The second principal component is the column of U corresponding to the second largest singular value, and so on.\n",
    "\n",
    "This process can be repeated. Once we find the first principal component, we can rerun the same equation with the added constraint that the new vector be orthogonal to the previously found vectors.\n",
    "\n",
    "\n",
    "# Summary of the algorithm\n",
    "1. Center the data matrix: $C = X - 1_n \\mu^T$ (where $1_n$ is a column vector of ones and $\\mu$ is the mean of each column)\n",
    "2. Compute SVD\n",
    "3. Find the principal components:  The first k principal components are the first k columns of V\n",
    "4. Transform the data. The transformed data is simply the first k columns of U.\n",
    "\n",
    "---"
   ],
   "id": "f60e0f22798614ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Nonlinear Featurization via K-means model stacking\n",
    "\n",
    "PCA is very useful when the data lies in a linear subspace like a flat pancake. But what if the data forms a more complicated shape ?!\n",
    "\n",
    "***nonlinear dimensionality reduction*** assumes that the data lies on a low-dimensional manifold embedded in the high-dimensional space. The goal is to find a mapping from the high-dimensional space to the low-dimensional manifold. For example, consider a paper (2D plane) that is rolled up into a cylinder (3D manifold). The paper is the low-dimensional manifold embedded in the high-dimensional space.\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "a3695f3fbc06e5c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Automating the Featurizer\n",
    "\n",
    "Manual Feature Extraction:\n",
    "- SIFT - Scale Invariant Feature Transform: Detects and describes local features in images (keypoints) that are invariant to scale and rotation.\n",
    "- HOG - Histogram of Oriented Gradients: Feature descriptor used in computer vision and image processing for the purpose of object detection. It counts occurrences of gradient orientation in localized portions of an image.\n",
    "\n",
    "***Image Gradient*** - the difference in value between neighboring pixels.\n",
    "- Horizontal\n",
    "- Vertical\n",
    "\n",
    "and compose them into a single vector.\n",
    "\n",
    "***Convolution*** - a mathematical operation that takes two functions and produces a third function. It is used in image processing to apply filters to images. It involves flipping the filter and taking the inner product with a small patch of the image, then moving to the\n",
    "next patch.\n",
    "\n",
    "Gradient:\n",
    "$$\n",
    "\\nabla I(i, j) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial I}{\\partial x} \\\\\n",
    "\\frac{\\partial I}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "I(i+1, j) - I(i-1, j) \\\\\n",
    "I(i, j+1) - I(i, j-1)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ],
   "id": "77b4693edc943635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T13:37:14.852231Z",
     "start_time": "2025-02-04T13:37:14.829273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import data, color\n",
    "\n",
    "image = color.rgb2gray(data.chelsea())\n",
    "\n",
    "# Compute the horizontal gradient\n",
    "gx = np.empty(image.shape, dtype=np.double)\n",
    "gx[:, 0] = 0\n",
    "gx[:, -1] = 0\n",
    "gx[:, 1:-1] = image[:, :-2] - image[:, 2:]\n",
    "\n",
    "# Compute the vertical gradient\n",
    "gy = np.empty(image.shape, dtype=np.double)\n",
    "gy[0, :] = 0\n",
    "gy[-1, :] = 0\n",
    "gy[1:-1, :] = image[:-2, :] - image[2:, :]\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(5, 10), sharex=True, sharey=True)\n",
    "ax1.axis('off')\n",
    "ax1.imshow(image, cmap=plt.cm.gray)\n",
    "ax1.set_title('Original image')\n",
    "ax1.set_adjustable('box-forced')\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.imshow(gx, cmap=plt.cm.gray)\n",
    "ax2.set_title('Horizontal gradients')\n",
    "ax2.set_adjustable('box-forced')\n",
    "\n",
    "ax3.axis('off')\n",
    "ax3.imshow(gy, cmap=plt.cm.gray)\n",
    "ax3.set_title('Vertical gradients')\n",
    "ax3.set_adjustable('box-forced')\n",
    "\n",
    "plt.show()"
   ],
   "id": "7f43ef1c54b194b1",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data, color\n\u001B[1;32m      5\u001B[0m image \u001B[38;5;241m=\u001B[39m color\u001B[38;5;241m.\u001B[39mrgb2gray(data\u001B[38;5;241m.\u001B[39mchelsea())\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Compute the horizontal gradient\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'skimage'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Magnitude: Euclidean norm of the gradient vector indicating how much the pixel intensity changes in that direction.\n",
    "\n",
    "Magnitude = $\\sqrt{(\\frac{\\partial I}{\\partial x})^2 + (\\frac{\\partial I}{\\partial y})^2}$\n",
    "\n",
    "orientation = $\\arctan(\\frac{\\partial I}{\\partial y}, \\frac{\\partial I}{\\partial x})$\n",
    "\n",
    "# HOG - Gradient Orientation Histograms\n",
    "\n",
    "1. Divide 0°–360° into equal-sized bins. For example, 0°–40°, 40°-80, ...\n",
    "2. Divide the image into cells. For example, 8x8 pixels.\n",
    "3. Compute the gradient orientation and magnitude for each block.\n",
    "4. equal-sized bins [orientation] += magnitude\n",
    "\n",
    "\n",
    "# SIFT\n",
    "Here, we want to avoid sudden changes in the feature descriptor resulting from small changes in the position of the image window. so it downweights gradients that come from the edges of the neighborhood using a Gaussian distance function measured from the window center.\n",
    "\n",
    "In other words, the gradient magnitude is multiplied by\n",
    "$$\n",
    "\\frac{1}{2\\pi\\sigma^2} e^{-\\frac{||p - p_0||^2}{2\\sigma^2}}\n",
    "$$\n",
    "where p is the location of the pixel that generated the gradient, p0 is the location of the center of the image neighborhood, and σ, the width of the Gaussian, is set to one-half the radius of the neighborhood.\n",
    "\n",
    "- 16x16 pixel window\n",
    "- 8 orientations\n",
    "- 4x4 cells making 4x4x8=128 features"
   ],
   "id": "a3f91b9665d0122d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
